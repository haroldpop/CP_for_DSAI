{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "92dd89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "92394dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "53e1e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=500, random_state=1)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8ef63041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5635bb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709.1962086421661"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log((1 - 1e-308) / 1e-308 ) #the smallest value of the error function that numpy can calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "57832ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.97      0.97        79\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = X_train.shape[0]\n",
    "S = 20\n",
    "stump_params = {'max_depth': 1, 'max_leaf_nodes': 2}\n",
    "models = [DecisionTreeClassifier(**stump_params) for _ in range(S)]\n",
    "\n",
    "#initially, we set our weight to 1/m\n",
    "W = np.full(m, 1/m)\n",
    "\n",
    "#keep collection of a_j\n",
    "a_js = np.zeros(S)\n",
    "\n",
    "#threshold for the calculation of the error\n",
    "epsilon = 1e-50\n",
    "\n",
    "#definition of eta\n",
    "eta = 1/2\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    \n",
    "    #train weak learner\n",
    "    model.fit(X_train, y_train, sample_weight = W)\n",
    "    \n",
    "    #compute the errors\n",
    "    yhat = model.predict(X_train) \n",
    "    err = W[(yhat != y_train)].sum()\n",
    "        \n",
    "    #compute the predictor weight a_j\n",
    "    #if predictor is doing well, a_j will be big\n",
    "    if err < epsilon :\n",
    "        err += 1e-20\n",
    "    a_j = np.log ((1 - err) / err) * eta\n",
    "    a_js[j] = a_j\n",
    "    \n",
    "    #update sample weight; divide sum of W to normalize\n",
    "    W = (W * np.exp(-a_j * y_train * yhat)) \n",
    "    W = W / sum (W)\n",
    "    \n",
    "        \n",
    "#make weighted predictions\n",
    "Hx = 0\n",
    "for i, model in enumerate(models):\n",
    "    yhat = model.predict(X_test)\n",
    "    Hx += a_js[i] * yhat\n",
    "    \n",
    "yhat = np.sign(Hx)\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838b89a",
   "metadata": {},
   "source": [
    "AdaBoost model seems solid to the changes of values for eta when this one is inferior of 1, but sensitive when the value of eta is superior than 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a16098d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stump:\n",
    "\n",
    "    def __init__(self, eta):\n",
    "        self.threshold = None\n",
    "        self.feature = None\n",
    "    \n",
    "    def fit(self, X, y, sample_weight) :\n",
    "        n_samples, n_features = X.shape\n",
    "        best_err = (sample_weight[(-y != y)].sum())*(1 / n_samples) #initialization with the biggest error we could find\n",
    "        for feature in range(n_features) :\n",
    "            sort_feature_sample = np.sort(X[:, feature])\n",
    "            for sample in range(n_samples - 1) :\n",
    "                if sort_feature_sample[sample] == sort_feature_sample[sample + 1] :\n",
    "                    continue\n",
    "                else :\n",
    "                    threshold = (sort_feature_sample[sample] + sort_feature_sample[sample + 1]) / 2\n",
    "                    yhat = np.ones(n_samples)\n",
    "                    yhat[ X[:, feature] < threshold ] = -1\n",
    "                    err = (sample_weight[(yhat != y)].sum())*(1 / n_samples)\n",
    "                \n",
    "                    if err < best_err :\n",
    "                        best_err = err\n",
    "                        threshold_ix = threshold\n",
    "                        feature_ix = feature\n",
    "        \n",
    "        self.threshold = threshold_ix\n",
    "        self.feature = feature_ix\n",
    "        \n",
    "    def predict(self, X) :\n",
    "        if self.feature is not None :\n",
    "            yhat = np.ones(X.shape[0])\n",
    "            yhat[ X[:, self.feature] < self.threshold ] = -1\n",
    "            return yhat\n",
    "        else :\n",
    "            raise ValueError('There is something wrong')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0d5c36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost :\n",
    "    def __init__(self, S, eta):\n",
    "        self.S = S\n",
    "        self.eta = eta\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        \n",
    "        W = np.full(m, 1/m)\n",
    "        classifiers = []\n",
    "        a_js = np.zeros(self.S)\n",
    "        \n",
    "        for k in range(self.S) :\n",
    "            stump = Stump(self.eta)\n",
    "            stump.fit(X, y, W)\n",
    "            yhat = stump.predict(X)\n",
    "            err = W[(yhat != y)].sum()\n",
    "            \n",
    "            if err < epsilon :\n",
    "                err += 1e-20\n",
    "            a_j = np.log ((1 - err) / err) * self.eta\n",
    "            a_js[k] = a_j\n",
    "\n",
    "            #update sample weight; divide sum of W to normalize\n",
    "            W = (W * np.exp(-a_j * y * yhat)) \n",
    "            W = W / sum (W)\n",
    "            classifiers.append(stump)\n",
    "        self.a_js = a_js\n",
    "        self.classifiers = classifiers\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Hx = 0\n",
    "        a_js = self.a_js\n",
    "        for i, model in enumerate(self.classifiers):\n",
    "            yhat = model.predict(X_test)\n",
    "            Hx += a_js[i] * yhat\n",
    "\n",
    "        yhat = np.sign(Hx)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "97e54799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.99      0.97        79\n",
      "           1       0.99      0.96      0.97        71\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = AdaBoost(20, 1/2)\n",
    "exp.fit(X_train, y_train)\n",
    "yhat = exp.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
